"""SWE-bench integration for evaluating the harness."""

import json
import asyncio
import subprocess
import tempfile
import shutil
from pathlib import Path
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
from datetime import datetime
import os

try:
    from datasets import load_dataset
    HAS_DATASETS = True
except ImportError:
    HAS_DATASETS = False


@dataclass
class SWEBenchTask:
    """A single SWE-bench task."""
    
    instance_id: str
    repo: str
    base_commit: str
    problem_statement: str
    hints_text: str
    patch: str  # Gold patch for evaluation
    test_patch: str
    version: str
    environment_setup_commit: str = ""
    fail_to_pass: List[str] = field(default_factory=list)
    pass_to_pass: List[str] = field(default_factory=list)
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "SWEBenchTask":
        """Create from dictionary."""
        return cls(
            instance_id=data.get("instance_id", ""),
            repo=data.get("repo", ""),
            base_commit=data.get("base_commit", ""),
            problem_statement=data.get("problem_statement", ""),
            hints_text=data.get("hints_text", ""),
            patch=data.get("patch", ""),
            test_patch=data.get("test_patch", ""),
            version=data.get("version", ""),
            environment_setup_commit=data.get("environment_setup_commit", ""),
            fail_to_pass=data.get("FAIL_TO_PASS", []) or [],
            pass_to_pass=data.get("PASS_TO_PASS", []) or [],
        )


@dataclass
class SWEBenchResult:
    """Result of running a SWE-bench task."""
    
    instance_id: str
    repo: str
    success: bool
    resolved: bool  # Did it pass the tests?
    model_patch: str  # The patch generated by the agent
    gold_patch: str
    error: Optional[str] = None
    duration_seconds: float = 0.0
    test_results: Dict[str, Any] = field(default_factory=dict)
    agent_output: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return {
            "instance_id": self.instance_id,
            "repo": self.repo,
            "success": self.success,
            "resolved": self.resolved,
            "model_patch": self.model_patch,
            "gold_patch": self.gold_patch,
            "error": self.error,
            "duration_seconds": self.duration_seconds,
            "test_results": self.test_results,
            "agent_output": self.agent_output,
        }


class SWEBenchRunner:
    """Runner for SWE-bench tasks."""
    
    def __init__(
        self,
        agent,
        workspace_base: Optional[Path] = None,
        timeout_seconds: int = 600,
        dataset_name: str = "princeton-nlp/SWE-bench_Lite",
    ):
        """Initialize the runner.
        
        Args:
            agent: The Agent instance to use.
            workspace_base: Base directory for task workspaces.
            timeout_seconds: Timeout for each task.
            dataset_name: HuggingFace dataset name.
        """
        self.agent = agent
        self.workspace_base = workspace_base or Path(tempfile.gettempdir()) / "swebench"
        self.timeout_seconds = timeout_seconds
        self.dataset_name = dataset_name
        self.results: List[SWEBenchResult] = []
    
    def load_tasks(
        self,
        split: str = "test",
        max_tasks: Optional[int] = None,
        instance_ids: Optional[List[str]] = None,
    ) -> List[SWEBenchTask]:
        """Load tasks from the SWE-bench dataset.
        
        Args:
            split: Dataset split ("test", "dev", etc.)
            max_tasks: Maximum number of tasks to load.
            instance_ids: Specific instance IDs to load.
        
        Returns:
            List of SWEBenchTask objects.
        """
        if not HAS_DATASETS:
            raise ImportError(
                "datasets library required. Install with: pip install datasets"
            )
        
        dataset = load_dataset(self.dataset_name, split=split)
        
        tasks = []
        for item in dataset:
            task = SWEBenchTask.from_dict(item)
            
            if instance_ids and task.instance_id not in instance_ids:
                continue
            
            tasks.append(task)
            
            if max_tasks and len(tasks) >= max_tasks:
                break
        
        return tasks
    
    def _clone_repo(self, task: SWEBenchTask, workspace: Path) -> bool:
        """Clone the repository and checkout the base commit."""
        try:
            # Clone the repo
            repo_url = f"https://github.com/{task.repo}.git"
            result = subprocess.run(
                ["git", "clone", "--depth", "100", repo_url, str(workspace)],
                capture_output=True,
                text=True,
                timeout=300,
            )
            
            if result.returncode != 0:
                # Try full clone if shallow fails
                subprocess.run(
                    ["git", "clone", repo_url, str(workspace)],
                    capture_output=True,
                    text=True,
                    timeout=600,
                )
            
            # Checkout the base commit
            subprocess.run(
                ["git", "checkout", task.base_commit],
                cwd=workspace,
                capture_output=True,
                text=True,
            )
            
            return True
        except Exception as e:
            print(f"Failed to clone repo: {e}")
            return False
    
    def _get_diff(self, workspace: Path) -> str:
        """Get the git diff of changes made by the agent."""
        result = subprocess.run(
            ["git", "diff"],
            cwd=workspace,
            capture_output=True,
            text=True,
        )
        return result.stdout
    
    def _apply_test_patch(self, task: SWEBenchTask, workspace: Path) -> bool:
        """Apply the test patch to add/modify tests."""
        if not task.test_patch:
            return True
        
        try:
            patch_file = workspace / "test.patch"
            patch_file.write_text(task.test_patch)
            
            result = subprocess.run(
                ["git", "apply", "test.patch"],
                cwd=workspace,
                capture_output=True,
                text=True,
            )
            
            patch_file.unlink()
            return result.returncode == 0
        except Exception:
            return False
    
    def _run_tests(
        self,
        task: SWEBenchTask,
        workspace: Path,
    ) -> Dict[str, Any]:
        """Run the tests for a task."""
        results = {
            "fail_to_pass": {},
            "pass_to_pass": {},
            "all_passed": False,
        }
        
        # Apply test patch first
        self._apply_test_patch(task, workspace)
        
        def run_test(test_name: str) -> bool:
            """Run a single test and return pass/fail."""
            try:
                result = subprocess.run(
                    ["python", "-m", "pytest", test_name, "-x", "-v"],
                    cwd=workspace,
                    capture_output=True,
                    text=True,
                    timeout=120,
                )
                return result.returncode == 0
            except Exception:
                return False
        
        # Run fail_to_pass tests (these should pass after fix)
        all_f2p_passed = True
        for test in task.fail_to_pass:
            passed = run_test(test)
            results["fail_to_pass"][test] = passed
            if not passed:
                all_f2p_passed = False
        
        # Run pass_to_pass tests (these should still pass)
        all_p2p_passed = True
        for test in task.pass_to_pass:
            passed = run_test(test)
            results["pass_to_pass"][test] = passed
            if not passed:
                all_p2p_passed = False
        
        results["all_passed"] = all_f2p_passed and all_p2p_passed
        return results
    
    def _build_prompt(self, task: SWEBenchTask, workspace: Path) -> str:
        """Build the prompt for the agent."""
        return f"""You are working on the repository: {task.repo}

The workspace is at: {workspace}

## Problem Statement

{task.problem_statement}

{f"## Hints{chr(10)}{task.hints_text}" if task.hints_text else ""}

## Instructions

1. First, explore the repository structure to understand the codebase.
2. Find the relevant files that need to be modified.
3. Understand the issue described in the problem statement.
4. Make the necessary code changes to fix the issue.
5. Make sure your changes don't break existing functionality.

Focus on making minimal, targeted changes that fix the issue. Do not make unnecessary refactoring or style changes.
"""
    
    async def run_task(self, task: SWEBenchTask) -> SWEBenchResult:
        """Run a single SWE-bench task.
        
        Args:
            task: The task to run.
        
        Returns:
            SWEBenchResult with the outcome.
        """
        start_time = datetime.now()
        workspace = self.workspace_base / task.instance_id
        
        # Clean up any existing workspace
        if workspace.exists():
            shutil.rmtree(workspace)
        workspace.mkdir(parents=True)
        
        try:
            # Clone the repository
            if not self._clone_repo(task, workspace):
                return SWEBenchResult(
                    instance_id=task.instance_id,
                    repo=task.repo,
                    success=False,
                    resolved=False,
                    model_patch="",
                    gold_patch=task.patch,
                    error="Failed to clone repository",
                )
            
            # Update agent workspace
            self.agent.config.workspace_path = workspace
            
            # Build prompt and run agent
            prompt = self._build_prompt(task, workspace)
            
            try:
                agent_output = await asyncio.wait_for(
                    self.agent.run(prompt),
                    timeout=self.timeout_seconds,
                )
            except asyncio.TimeoutError:
                return SWEBenchResult(
                    instance_id=task.instance_id,
                    repo=task.repo,
                    success=False,
                    resolved=False,
                    model_patch=self._get_diff(workspace),
                    gold_patch=task.patch,
                    error="Agent timed out",
                    duration_seconds=(datetime.now() - start_time).total_seconds(),
                )
            
            # Get the diff (patch) generated by the agent
            model_patch = self._get_diff(workspace)
            
            # Run tests
            test_results = self._run_tests(task, workspace)
            
            duration = (datetime.now() - start_time).total_seconds()
            
            return SWEBenchResult(
                instance_id=task.instance_id,
                repo=task.repo,
                success=True,
                resolved=test_results.get("all_passed", False),
                model_patch=model_patch,
                gold_patch=task.patch,
                duration_seconds=duration,
                test_results=test_results,
                agent_output=agent_output,
            )
        
        except Exception as e:
            return SWEBenchResult(
                instance_id=task.instance_id,
                repo=task.repo,
                success=False,
                resolved=False,
                model_patch="",
                gold_patch=task.patch,
                error=str(e),
                duration_seconds=(datetime.now() - start_time).total_seconds(),
            )
        
        finally:
            # Optionally clean up workspace
            # shutil.rmtree(workspace, ignore_errors=True)
            pass
    
    async def run_all(
        self,
        tasks: List[SWEBenchTask],
        parallel: int = 1,
    ) -> List[SWEBenchResult]:
        """Run all tasks.
        
        Args:
            tasks: List of tasks to run.
            parallel: Number of parallel tasks (default 1 for sequential).
        
        Returns:
            List of results.
        """
        if parallel == 1:
            # Sequential execution
            for i, task in enumerate(tasks):
                print(f"\n{'='*60}")
                print(f"Task {i+1}/{len(tasks)}: {task.instance_id}")
                print(f"Repo: {task.repo}")
                print(f"{'='*60}\n")
                
                result = await self.run_task(task)
                self.results.append(result)
                
                status = "✅ RESOLVED" if result.resolved else "❌ NOT RESOLVED"
                print(f"\nResult: {status}")
                if result.error:
                    print(f"Error: {result.error}")
        else:
            # Parallel execution (be careful with rate limits)
            semaphore = asyncio.Semaphore(parallel)
            
            async def run_with_semaphore(task):
                async with semaphore:
                    return await self.run_task(task)
            
            self.results = await asyncio.gather(
                *[run_with_semaphore(task) for task in tasks]
            )
        
        return self.results
    
    def get_summary(self) -> Dict[str, Any]:
        """Get a summary of all results."""
        if not self.results:
            return {"total": 0, "resolved": 0, "resolution_rate": 0.0}
        
        resolved = sum(1 for r in self.results if r.resolved)
        successful = sum(1 for r in self.results if r.success)
        total = len(self.results)
        
        return {
            "total": total,
            "resolved": resolved,
            "successful": successful,
            "failed": total - successful,
            "resolution_rate": resolved / total if total > 0 else 0.0,
            "success_rate": successful / total if total > 0 else 0.0,
            "avg_duration": sum(r.duration_seconds for r in self.results) / total,
        }
    
    def save_results(self, output_path: Path):
        """Save results to a JSON file."""
        data = {
            "summary": self.get_summary(),
            "results": [r.to_dict() for r in self.results],
            "timestamp": datetime.now().isoformat(),
            "dataset": self.dataset_name,
        }
        
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w") as f:
            json.dump(data, f, indent=2)
        
        print(f"Results saved to {output_path}")


async def run_swebench_evaluation(
    config,
    max_tasks: int = 10,
    dataset: str = "princeton-nlp/SWE-bench_Lite",
    output_dir: Path = None,
    instance_ids: List[str] = None,
):
    """Run SWE-bench evaluation.
    
    Args:
        config: Agent configuration.
        max_tasks: Maximum number of tasks.
        dataset: Dataset name.
        output_dir: Output directory for results.
        instance_ids: Specific instances to run.
    
    Returns:
        Summary dictionary.
    """
    from .agent import Agent
    
    output_dir = output_dir or Path("swebench_results")
    
    agent = Agent(config, max_iterations=30)
    runner = SWEBenchRunner(
        agent=agent,
        dataset_name=dataset,
    )
    
    print(f"Loading tasks from {dataset}...")
    tasks = runner.load_tasks(
        max_tasks=max_tasks,
        instance_ids=instance_ids,
    )
    print(f"Loaded {len(tasks)} tasks")
    
    print("\nRunning evaluation...")
    await runner.run_all(tasks)
    
    summary = runner.get_summary()
    
    print("\n" + "="*60)
    print("EVALUATION SUMMARY")
    print("="*60)
    print(f"Total tasks: {summary['total']}")
    print(f"Resolved: {summary['resolved']}")
    print(f"Resolution rate: {summary['resolution_rate']*100:.1f}%")
    print(f"Average duration: {summary['avg_duration']:.1f}s")
    print("="*60)
    
    # Save results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    runner.save_results(output_dir / f"results_{timestamp}.json")
    
    return summary
